import src.DeepGaze.deepgaze_pytorch as deepgaze_pytorch
import numpy as np
from scipy.misc import face
from scipy.ndimage import zoom
from scipy.special import logsumexp
import torch

import matplotlib.pyplot as plt
from tqdm import tqdm

import argparse 
import cv2 

def gen_saliency(image,model,DEVICE, channel_loc = 0 ):
    '''
    Generate saliency map for input image using DeepGaze 
    
    Args:
        image: input image 
        channel_loc: index of the channel dimesion of the image
    
    Returns:
        log_density_prediction: generated saliency map 
    
    '''  
    model = model 

    centerbias_template = np.load('../src/DeepGaze/centerbias_mit1003.npy')
    
    # Here Image is supposed to be [H x W x C] 
    # But NSD images are [C x H x W] 
        
    if channel_loc == 0:
        centerbias = zoom(centerbias_template, (image.shape[1]/centerbias_template.shape[0], image.shape[2]/centerbias_template.shape[1]), order=0, mode='nearest')
        image_tensor = torch.tensor([image]).to(DEVICE)

    elif channel_loc == 2: 
        centerbias = zoom(centerbias_template, (image.shape[0]/centerbias_template.shape[0], image.shape[1]/centerbias_template.shape[1]), order=0, mode='nearest')
        image_tensor = torch.tensor([image.transpose(2, 0, 1)]).to(DEVICE) # for the face example
    
    centerbias -= logsumexp(centerbias)
    
    # create image tensor with H x W x C dimensions 
    
    # if the image is of the shape [W x H x C] --> H x W x C    
    centerbias_tensor = torch.tensor([centerbias]).to(DEVICE)
    log_density_prediction = model(image_tensor, centerbias_tensor)
    
    return log_density_prediction 


def plot_saliency(image, log_density_prediction, channel_loc = 0):
    '''
    Plot saliency maps using the log density prediction generated by DeepGaze
    
    Args:
        image: input image
        log_density_prediction: saliency map generated with DeepGaze
        channel_loc: index of the channel dimesion of the image
    
    Returns:
        None
    '''
    f, axs = plt.subplots(nrows=1, ncols=2, figsize=(20,6))
    if channel_loc == 0:
        axs[0].imshow(image.transpose((1,2,0)), cmap='gray', interpolation='None')
    elif channel_loc == 2:
         axs[0].imshow(image, cmap='gray', interpolation='None')
        
    axs[1].matshow(log_density_prediction.detach().cpu().numpy()[0, 0])
    axs[0].set_axis_off()
    axs[1].set_axis_off()
    

def gen_and_plot_saliency(model, image, DEVICE, channel_loc = 0):
    '''
    Generate and plot saliency maps 
    
    Args:
        model: DeepGaze model instance
        image: input image
        channel_loc: index of the channel dimesion of the image
        
    Returns: 
        None:
    '''
    log_density_prediction = gen_saliency(image,model,DEVICE, channel_loc )
    plot_saliency(image, log_density_prediction, channel_loc)


    
def gen_saliency_batch(image,model, DEVICE, channel_loc = 0):
    '''
    Generate saliency map using DeepGaze for mutple batches at once, if the input images are in batches 
    
    '''
    model = model 

    centerbias_template = np.load('../src/DeepGaze/centerbias_mit1003.npy')
    
    # Here Image is supposed to be [H x W x C] 
    # But NSD images are [C x H x W] 
        
    if channel_loc == 0:
        centerbias = zoom(centerbias_template, (image.shape[1]/centerbias_template.shape[0], 
                                                image.shape[2]/centerbias_template.shape[1]), order=0, mode='nearest')
        image_tensor = torch.tensor(image).to(DEVICE)

    elif channel_loc == 2: 
        centerbias = zoom(centerbias_template, (image.shape[1]/centerbias_template.shape[0], 
                                                image.shape[2]/centerbias_template.shape[1]), order=0, mode='nearest')
        image_tensor = torch.tensor(image.transpose(0,3,1,2)).to(DEVICE) # for the face example

    centerbias -= logsumexp(centerbias)
    
    # create image tensor with H x W x C dimensions 
    # if the image is of the shape [W x H x C] --> H x W x C     
    centerbias_tensor = torch.tensor([centerbias]).to(DEVICE)
    log_density_prediction = model(image_tensor, centerbias_tensor)

    return log_density_prediction 


def gen_S_matrix_batched(model, DEVICE,  img_indices, dataset,  default_stimuli = True, batch_size = 8):
    '''
    Args:
        default_stimuli:
            True: in the case of generateion for regular images without a concatenated noise (normal case) 
            False: in the case of generateion for concatenated noise images (for saliency scores) 
    '''
    
    batch_size = batch_size    
    
    
    if default_stimuli: 
        
        num_pixels = 425
        S_mat_full = torch.ones((1,num_pixels,num_pixels))

        for indices in tqdm(range(0,len(img_indices), batch_size)):
            
            chunk = img_indices[indices:indices+batch_size]
            images = dataset[chunk] # [batch_size x 425 x 425 x 3] 


            sal_maps = gen_saliency_batch(images,model, DEVICE, channel_loc = 2)[:,0].detach().cpu() # batch_size, 425, 425

            S_mat_full = torch.cat((S_mat_full.cpu(), sal_maps), dim = 0) 
            
    
    else:
        # When generating the concatenated stimuli (with Pink noise) 
        img_width = 425 * 2
        img_height = 425
        
        S_mat_full = torch.ones((1 , img_height, img_width )) 
        
        for indices in tqdm(range(0,len(img_indices), batch_size)):

            chunk = img_indices[indices:indices+batch_size]
            
            
            images = create_concat_dataset_batched(dataset, chunk)      # [batch_size x 425 x 425 x 3] 
  
            sal_maps = gen_saliency_batch(images,model, DEVICE, channel_loc = 2)[:,0].detach().cpu() # batch_size, 425, 425
            # print(S_mat_full.shape, sal_maps.shape) 
            S_mat_full = torch.cat((S_mat_full.cpu(), sal_maps), dim = 0) 
        
    return S_mat_full


def create_concat_dataset_batched(img_arr, batch):
    '''
    Generate an image dataset from the COCO stimuli where each corresponding COCO image is concatenated
    with the pink noise image. This will be used subsequently to generate relative saliency scores for the 
    images 
    '''
    num_imgs = len(batch)
    img_height = img_arr.shape[1]
    img_width = img_arr.shape[2]
    num_channel = img_arr.shape[3]
    
    img_dataset = np.ones((img_height, img_width*2, num_channel), dtype = 'uint8') # 425 , 850, 3     
    
    for idx in batch: # generates one extra image 
        concat_img = generate_concat_imgs(img_arr, idx,idx, pass_noise=True)
        img_dataset = np.vstack((img_dataset, concat_img))
        
       
    reshaped = img_dataset.reshape((num_imgs + 1 , img_height , img_width * 2 , num_channel))
        
    return reshaped[1:, :, :, :]

def generate_concat_imgs(dataset, idx1,idx2, pass_noise = False):
    # Note: The dimensions of the images must be same to use this method
    
    image_left = dataset[idx1]
    
    if pass_noise:
        image_right = (generate_pink_noise(425,425)*255).astype(np.uint8)
    else: 
        image_right = dataset[idx2] 

        
    final_image = np.concatenate((image_left, image_right), axis = 1) 
    
    return final_image


def generate_pink_noise(height , width): 
    np.random.seed(42) 
    whitenoise = np.random.uniform(0, 1, (height, width))

    ft_arr = np.fft.fftshift(np.fft.fft2(whitenoise))

    _x, _y = np.mgrid[0:ft_arr.shape[0], 0:ft_arr.shape[1]]
    f = np.hypot(_x - ft_arr.shape[0] / 2, _y - ft_arr.shape[1] / 2)

    pink_ft_arr = ft_arr / f

    pink_ft_arr = np.nan_to_num(pink_ft_arr, nan=0, posinf=0, neginf=0)
    pinknoise = np.fft.ifft2(np.fft.ifftshift(pink_ft_arr)).real
        
    stacked_img = np.stack((pinknoise,)*3, axis=-1)
    stacked_img_normalized = stacked_img - stacked_img.min()
    stacked_img_normalized /= stacked_img_normalized.max()

    return stacked_img_normalized



def gen_corr_w_beta(roi, S_mat):
    
    print(f"Computing correlation for {roi}\n")

    K_mat = np.load(DATA_PATH+f"/K_mat_subj1_{roi}.npy.npz")
    K_mat = K_mat['arr_0']
    print(f"K matrix: {K_mat.shape}")
    
    betas_mat = np.load(DATA_PATH+f"/betas_full_{roi}.npy")
          
    roi_valid_indices = np.load(DATA_PATH+f"/{roi}_valid_indices_subj1.npy")
    print(f"Betas matrix before valid: {betas_mat.shape}")
    print(f"Valid indices shape: {roi_valid_indices.shape}")
    
    # index the betas for the corresponding voxels that are valid 
    betas_val = betas_mat[:,roi_valid_indices] 
    betas_val = betas_val[1:,:] # remove the first dummy row 
    print(f"Betas matrix: {betas_val.shape}")
    
    # Mulitply the S and K matrix to get Y hat 
    Y_hat = S_mat @ K_mat.T
    print(f"Y_hat matrix: {Y_hat.shape}\n\n")      
    
    # Correlate between betas_valid and Y_hat 
    corr_mat = np.corrcoef(betas_val,Y_hat,rowvar = False) # Example: (27750, 4165) x (27750, 4165) for V1 

    return corr_mat



def generate_corr_full(roi_list, S_mat):
    '''
    Generate the correlation coeff for the given list of ROIs and return it as a dataframe 
    '''
    corr_vals = {}
    
    for roi in roi_list:
        corr_mat_roi = gen_corr_w_beta(roi,S_mat) 
        rel_size = (corr_mat_roi.shape[0])// 2
        corr_mat_rel_roi = corr_mat_roi[rel_size:,:rel_size]
        corr_vals[roi] = corr_mat_rel_roi.diagonal() 
        
    
    # Create a main dataframe to which all smaller dataframes (for each ROI) will be appended 
    df_main = pd.DataFrame(columns = ['ROI','corr_coeff'])
    
    for key,val in corr_vals.items(): # Key: ROI, value: array of correlations
        roi_as_list = [key] * val.shape[0] # duplicate the roi string so that it's in the format we want 
        corr_val_list = val
        temp_df = pd.DataFrame({'ROI': roi_as_list, 'corr_coeff':corr_val_list})   
        df_main = pd.concat((df_main, temp_df))
        
    return df_main

def generate_maps_dict(map_type, img_indices, image_data_73k, num_pixels = 425, kernel_size = None):
    '''
    Generate a dictionary containing the required low-level feature map for an image 
    
    '''
    maps_dict = {} 
    
    for idx in tqdm(img_indices): 
        
        if idx in maps_dict: 
            continue
        
        else: 
            image = image_data_73k[idx]
            
            if map_type == 'edge':
                img_map = generate_edge_map_single(image, kernel_size) 
                
            elif map_type == 'saturation':
                img_map = generate_saturation_map_single(image) 
                
            elif map_type == 'luminance':
                img_map = generate_luminance_map_single(image)
                
            maps_dict[idx] = img_map
        
    return maps_dict


def convert_dict_to_S_mat(map_dict, img_indices, num_pixels = 425): 
    '''
    Convert the dictionary containing low-level feature maps into the S_mat format
    '''
    S_mat_full = np.ones([len(img_indices), num_pixels*num_pixels])

    for trial_num, idx in enumerate(tqdm(img_indices)):
        
        S_map_img = map_dict[idx] 
        
        S_map_long = S_map_img.reshape(-1, num_pixels * num_pixels)
        
        S_mat_full[trial_num, :] = S_map_long
        
    return S_mat_full


def generate_edge_map_single(image, kernel_size):
    
    if kernel_size == None: 
        raise ValueError("kernel_size not defined.") 
        
    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
    blurred = cv2.GaussianBlur(gray, (5,5),0) # 5x5 gaussian filter, to reduce noise 
    laplacian = cv2.Laplacian(blurred, kernel_size, cv2.CV_64F)
    edge_map = cv2.convertScaleAbs(laplacian) 

    return edge_map

def generate_saturation_map_single(img): 
    image_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) 
    saturation_map = image_hsv[:,:,1]
    
    return saturation_map

def generate_luminance_map_single(img): 
    image_hsv = cv2.cvtColor(img, cv2.COLOR_BGR2HSV) 
    luminance_map = image_hsv[:,:,2]
    
    return luminance_map